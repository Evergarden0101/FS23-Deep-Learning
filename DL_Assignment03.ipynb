{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Evergarden0101/FS23-Deep-Learning/blob/main/DL_Assignment03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYyOevvgNRMP"
      },
      "source": [
        "# Assignment 3: Universal Function Approximator\n",
        "\n",
        "\n",
        "The goal of this exercise is to train a two-layer fully-connected network to perform one-dimensional non-linear regression via gradient descent. To show the flexibility of the approach, three different functions will be approximated. First, the network and its gradient need to be implemented. Second, target data for three different functions will be generated. Finally, the training procedure will be applied to the data, and the resulting approximated function will be plotted together with the data samples.\n",
        "\n",
        "## Network Implementation\n",
        "\n",
        "A two-layer network is defined by parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer. In our case, since we have only a single input, we have $D=1$.\n",
        "For a given input $\\vec x = (1, x)^T$, the network is implemented in three steps:\n",
        "\n",
        "1. Compute the first layer output, aka, the activation: $\\vec a_- = \\mathbf W^{(1)} \\vec x$\n",
        "2. Apply the activation function for each element of $\\vec a_- : \\vec h_- = g(\\vec a_-)$ and prepend the bias neuron $h_0=1$ to arrive at $\\vec h$.\n",
        "3. Compute the output of the network: $y = \\vec w^{(2)}\\ ^T\\vec h$.\n",
        "\n",
        "### Task 1  \n",
        "Implement a function that returns the network output for a given input $\\vec x$ and parameters $\\Theta=(\\mathbf W^{(1)}, \\vec w^{(2)})$. Remember that the input of the function $\\vec x = (1, x)^T$. Also remember to prepend $h_0=1$ in your implementation.\n",
        "\n",
        "We use hyperbolic tangent $(\\tanh)$ as the activation function:\n",
        "\n",
        "\\begin{equation*}\n",
        "    \\tanh(a) = \\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}}\n",
        "\\end{equation*}\n",
        "\n",
        "Note:\n",
        "\n",
        "1. Use the `numpy` implemention of the hyperbolic tangent function.\n",
        "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
        "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4sCXWf4NRMS"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import math\n",
        "\n",
        "def network(x, Theta):\n",
        "    W1, w2 = Theta\n",
        "    a_ = numpy.dot(W1,x)\n",
        "    h_=numpy.tanh(a_)\n",
        "    h=h_\n",
        "    if w2.shape[0]!=h.shape[0]:\n",
        "      h = numpy.concatenate((numpy.array([1]),h_))\n",
        "    y = numpy.dot(w2.Th)\n",
        "    return  y,h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwkDlBk7NRMT"
      },
      "source": [
        "Test 1: Sanity Check\n",
        "----------------------------\n",
        "\n",
        "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below assures that the function from Task 1 actually returns $1$ for those weights.\n",
        "\n",
        "Note: your function should pass the test below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSMGLX0GNRMU",
        "outputId": "bf12ba60-4fa7-466d-97bb-797ace2526ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed\n"
          ]
        }
      ],
      "source": [
        "K = 20\n",
        "D = 1\n",
        "W1 = numpy.zeros((K, D+1))\n",
        "w2 = numpy.ones(K+1)\n",
        "x = numpy.random.rand(D+1)\n",
        "\n",
        "y, _ = network(x, (W1, w2))\n",
        "assert abs(1 - y) < 1e-6\n",
        "print(\"Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdiwJo97NRMU"
      },
      "source": [
        "## Gradient Implementation\n",
        "\n",
        "In order to perform gradient descent, we need to define a loss function. As provided in the lecture, the $\\mathcal J^{L_2}$ loss function is defined over a dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, that is defined as a list of tuples, as follows:\n",
        "\n",
        "$$\n",
        "   \\mathcal J^{L_2} = \\frac{1}{N}\\sum_{i=1}^N (y^{[n]}-t^{[n]})^2\n",
        "$$\n",
        "\n",
        "where $y^{[n]}$ is the output of the network from Task 1 when inputting $\\vec x^{[n]}$. Interestingly, however, we never explicitly need to compute the output of the loss function. It is only used to analytically compute the gradient as shown in the lecture.\n",
        "\n",
        "The gradient is composed of two items, one for each layer. Particularly, for a given dataset $X$, the gradient of loss $J^{L_2}$ is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "  \\frac{\\partial \\mathcal J}{\\partial w_{kd}^{(1)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} (1-h_{k}^{[n]}\\cdot h_{k}^{[n]}) x_{d}^{[n]}\\\\\n",
        "  \\frac{\\partial \\mathcal J}{\\partial w_{k}^{(2)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} h_{k}^{[n]}\n",
        "\\end{align}\n",
        "\n",
        "### Task 2\n",
        "Implement a function that returns the gradient as defined in $(1)$ and $(2)$ for a given dataset $X$, and given weights $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$. Make sure that both parts of the gradient are computed. \n",
        "\n",
        "Hint:\n",
        "\n",
        "1. Make use of the the function implemented in Task 1 where appropriate\n",
        "\n",
        "Note:\n",
        "\n",
        "  1. This is a slow implementation. We will see how to speed this up in the next lecture.\n",
        "  2. You can make use of `numpy.zeros` to initialize the gradient.\n",
        "  3. The outper product can be computed via `numpy.outer`.\n",
        "  4. Remember that we used the $\\tanh$ activation function in our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSTDEiHENRMV"
      },
      "outputs": [],
      "source": [
        "def gradient(X, Theta):\n",
        "  # split parameters for easier handling\n",
        "    W1, w2 = Theta\n",
        "  \n",
        "  # define gradient with respect to both parameters\n",
        "    dW1=0\n",
        "    dw2=0\n",
        "    N = len(X)\n",
        "  # iterate over dataset\n",
        "    for x, t in X:\n",
        "    # compute the gradient \n",
        "        y,h=network(x, Theta)\n",
        "        dw11=2/N*numpy.outer([(y-t)*w2*(1-h*h)],x)\n",
        "        dw22=2/N*(y-t)*w2*h\n",
        "        dW1=dW1+dw11\n",
        "        dw2=dw2+dw22\n",
        "\n",
        "  # anything else?\n",
        "\n",
        "  \n",
        "    return dW1, dw2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLqOkUutNRMV"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "The procedure of gradient descent is the repeated application of two steps.\n",
        " \n",
        "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$.\n",
        "2. The weights are updated by moving a small step $\\eta$ into the direction of the negative gradient:\n",
        "\n",
        "$$\n",
        "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
        "$$\n",
        "\n",
        "As stopping criterion, we select the number of training epochs to be 10000.\n",
        "\n",
        "### Task 3\n",
        "Implement a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$ and a given learning rate $\\eta$ and returns the optimized parameters $\\Theta^*$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yqO9jSjNRMV"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Theta, eta):\n",
        "    epochs = 10000\n",
        "    W1 = Theta[0]\n",
        "    w2 = Theta[1]\n",
        "  # perform iterative gradient descent\n",
        "    for epoch in range(epochs):\n",
        "    # compute the gradient\n",
        "        dW1, dw2=gradient(X, (W1,w2))\n",
        "    # update the parameters\n",
        "        W1=W1-eta*dW1\n",
        "        w2=w2-eta*dw2\n",
        "  # return optimized parameters\n",
        "    return W1,w2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVan57uYNRMW"
      },
      "source": [
        "## Generate Datasets\n",
        "\n",
        "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate:\n",
        "\n",
        "1. $X_1: t = \\sin(2x)$ for $x\\in[-2,2]$\n",
        "2. $X_2: t = e^{-2x^2}$ for $x\\in[-2,2]$\n",
        "3. $X_3: t = -x^5 - 3x^4 + 11x^3 + 27x^2 - 10x - 64$ for $x\\in[-4.5,3.5]$\n",
        "\n",
        "### Task 4\n",
        "\n",
        "Generate dataset $X_1$, for $N=50$ samples randomly drawn from range $x\\in[-2,2]$. \n",
        "Generate data $X_2$ for $N=30$ samples randomly drawn from range $x\\in[-2,2]$. \n",
        "Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4.5,3.5]$. \n",
        "Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$.\n",
        "\n",
        "Note:\n",
        "\n",
        "  1. You can use `numpy.random.uniform` to create uniformly distributed samples for $x$.\n",
        "  2. Make sure that $\\vec x = (1, x)^T$ for each sample.\n",
        "  3. You can make use of `numpy.sin`, `numpy.exp` and `numpy.pow` to compute target values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LheWqY-CNRMW"
      },
      "outputs": [],
      "source": [
        "x1_uni=numpy.random.uniform(-2,2,50)\n",
        "X1=[]\n",
        "for i in range(len(x1_uni)):\n",
        "    x1=numpy.append(numpy.array([1]),x1_uni[i])\n",
        "    X1.append((x1,numpy.sin(2*x1_uni[i])))\n",
        "\n",
        "x2_uni=numpy.random.uniform(-2,2,30)\n",
        "X2=[]\n",
        "for i in range(len(x2_uni)):\n",
        "    x2=numpy.append(numpy.array([1]),x2_uni[i])\n",
        "    X2.append((x2,numpy.exp(-2*numpy.power(x2_uni[i],2))))\n",
        "\n",
        "x3_uni=numpy.random.uniform(-4.5,3.5,200)\n",
        "X3=[]\n",
        "\n",
        "for i in range(len(x3_uni)):\n",
        "    x3=numpy.append(numpy.array([1]),x3_uni[i])\n",
        "    X3.append((x3,-numpy.power(x3_uni[i],5)-3*numpy.power(x3_uni[i],4)+11*numpy.power(x3_uni[i],3)+27*numpy.power(x3_uni[i],2)-10*x3_uni[i]-64))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHZhm-D7NRMW"
      },
      "source": [
        "### Test 2: Sanity Check\n",
        "\n",
        "The test case below assures that the elements of each generated dataset are tuples with two elements, that the first element ($\\vec x$) is a vector with two numbers, and that the second element ($t$) is a float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Qeu0fKNRMW",
        "outputId": "aef2a986-b495-441e-a220-5dc43ac31d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ],
      "source": [
        "assert all(\n",
        "    isinstance(x, (tuple,list)) and \n",
        "    len(x) == 2 and \n",
        "    isinstance(x[0], (tuple,list,numpy.ndarray)) and \n",
        "    len(x[0] == 2) and \n",
        "    isinstance(x[1], float)\n",
        "    for X in (X1, X2, X3)\n",
        "    for x in X\n",
        ")\n",
        "\n",
        "print('Test passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDMcXGnlNRMX"
      },
      "source": [
        "###### Function Approximation\n",
        "\n",
        "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
        "\n",
        "### Task 5 (theoretical question)\n",
        "\n",
        "When looking at the example plots in the exercise slides (exemplary solutions for the plotting Task 8), how many hidden neurons $K$ do we need in order to approximate the functions? Is there any difference\n",
        "1 between the three target functions?\n",
        "\n",
        "Answer:The number of hidden neurons required to approximate a function using a neural network depends on the complexity of the function and the desired level of accuracy.\n",
        "For the first function, 2-5 hidden neuron may be sufficient, as the function is relatively simple and has a smooth shape.\n",
        "\n",
        "For the second function , 5-10 hidden neurons may be required, as the function has a more complex shape with sharper peaks and valleys.\n",
        "\n",
        "For the third function , 30-40 hidden neurons may be required, as the function is highly nonlinear and has multiple local extrema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o5mrusZNRMX"
      },
      "outputs": [],
      "source": [
        "K1 = 4\n",
        "K2 = 8\n",
        "K3 = 35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R_A8SFkNRMX"
      },
      "source": [
        "### Task 6\n",
        "\n",
        "For each of the datasets, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ according to the number of hidden neurons estimated in Task 5.\n",
        "\n",
        "Note:\n",
        "\n",
        "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
        "  2. Make sure that the weight matrices are instantiated in the correct dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjSipl1qNRMX"
      },
      "outputs": [],
      "source": [
        "D=1\n",
        "Theta1 = (numpy.random.uniform(-1,1,(K1+1, D+1)),numpy.random.uniform(-1,1,K1+1))\n",
        "Theta2 = (numpy.random.uniform(-1,1,(K2+1, D+1)),numpy.random.uniform(-1,1,K2+1))\n",
        "Theta3 = (numpy.random.uniform(-1,1,(K3+1, D+1)),numpy.random.uniform(-1,1,K3+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jyySftvNRMX"
      },
      "source": [
        "### Task 7\n",
        "\n",
        "Call gradient descent function from Task 3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$ and a learning rate of $\\eta=0.1$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$ and the loss values.\n",
        "\n",
        "Optimize the learning rate $\\eta$ for each of the three functions. Do you see any differences? What are the best learning rates that you can find?\n",
        "\n",
        "WARNING: Depending on the implementation, this might run for several minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "J5i2pfe-NRMX",
        "outputId": "beb32eee-1154-48e1-dd69-c433822a0462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Function:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-7269b56cd9ad>:13: RuntimeWarning: overflow encountered in multiply\n",
            "  dw11=2/N*numpy.outer([(y-t)*w2*(1-h*h)],x)\n",
            "<ipython-input-3-7269b56cd9ad>:13: RuntimeWarning: invalid value encountered in multiply\n",
            "  dw11=2/N*numpy.outer([(y-t)*w2*(1-h*h)],x)\n",
            "<ipython-input-3-7269b56cd9ad>:14: RuntimeWarning: overflow encountered in multiply\n",
            "  dw22=2/N*(y-t)*w2*h\n",
            "<ipython-input-3-7269b56cd9ad>:16: RuntimeWarning: invalid value encountered in add\n",
            "  dw2=dw2+dw22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for eta=0.1 loss=nan\n",
            "for eta=0.01 loss=0.2806085573041569\n",
            "for eta=0.001 loss=0.39953627503705064\n",
            "for eta=0.0001 loss=0.5348252063157315\n",
            "for eta=1e-05 loss=0.8606437927718001\n",
            "Second Function:\n",
            "for eta=0.1 loss=9.453389000944833e-05\n",
            "for eta=0.01 loss=0.00597222823069191\n",
            "for eta=0.001 loss=0.025667917007543502\n",
            "for eta=0.0001 loss=0.17964957109928018\n",
            "for eta=1e-05 loss=0.25101904256257257\n",
            "Third Function:\n",
            "for eta=0.1 loss=nan\n",
            "for eta=0.01 loss=2155.5480630608563\n",
            "for eta=0.001 loss=62.80253297656384\n",
            "for eta=0.0001 loss=580.9297331775649\n",
            "for eta=1e-05 loss=2142.363761655508\n"
          ]
        }
      ],
      "source": [
        "print('First Function:')\n",
        "for k in range(5):\n",
        "    fun1_W1,fun1_w2=gradient_descent(X1, Theta1, eta=0.1/numpy.power(10,k))\n",
        "    loss1=0\n",
        "    for i in range(len(X1)):\n",
        "        loss1=loss1+1/len(x1_uni)*numpy.power(network(X1[i][0],(fun1_W1,fun1_w2))[0]-X1[i][1],2)\n",
        "    print('for eta='+str(0.1/numpy.power(10,k))+' loss='+str(loss1))\n",
        "print('Second Function:')\n",
        "for k in range(5):\n",
        "    fun2_W1,fun2_w2=gradient_descent(X2, Theta2, eta=0.1/numpy.power(10,k))\n",
        "    loss2=0\n",
        "    for i in range(len(X2)):\n",
        "        loss2=loss2+1/len(x2_uni)*numpy.power(network(X2[i][0],(fun2_W1,fun2_w2))[0]-X2[i][1],2)\n",
        "    print('for eta='+str(0.1/numpy.power(10,k))+' loss='+str(loss2))\n",
        "print('Third Function:')\n",
        "for k in range(5):\n",
        "    fun3_W1,fun3_w2=gradient_descent(X3, Theta3, eta=0.1/numpy.power(10,k))\n",
        "    loss3=0\n",
        "    for i in range(len(X3)):\n",
        "        loss3=loss3+1/len(x3_uni)*numpy.power(network(X3[i][0],(fun3_W1,fun3_w2))[0]-X3[i][1],2)\n",
        "    print('for eta='+str(0.1/numpy.power(10,k))+' loss='+str(loss3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, when we decrese the $\\eta$ by 10 times, the loss values become larger. The best learning rate is achieved when the loss value is the smallest and the best learning rate for each of the three functions is different. The best $\\eta$ for the first function is 0.1, the best $\\eta$ for the second function is also 0.1, but the best $\\eta$ for the third function is 0.001."
      ],
      "metadata": {
        "id": "QbrQw_igsvnk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0fwmtcBNRMY"
      },
      "source": [
        "## Data and Function Plotting\n",
        "\n",
        "### Task 8\n",
        "\n",
        "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$ and a defined range $R$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
        "\n",
        "Note:\n",
        "\n",
        "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
        "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
        "  3. Equidistant points can be obtained via `numpy.arange`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tgop4JtNRMY"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "def plot(X, Theta, R):\n",
        "  # first, plot data samples\n",
        "    x1=[]\n",
        "    y1=[]\n",
        "    for a in X:\n",
        "        x1.append(a[0][1])\n",
        "        y1.append(a[1])\n",
        "    pyplot.plot(x1, y1, \"rx\", label=\"Data\")\n",
        "\n",
        "  # define equidistant points from R[0] to R[1] to evaluate the network\n",
        "    n=100\n",
        "    x= numpy.arange(R[0],R[1],(R[1]-R[0])/n)\n",
        "    y=numpy.zeros(n)\n",
        "  # compute the network outputs for these values\n",
        "    for i in range(n):\n",
        "        x_temp=numpy.append(numpy.array([1]),x[i])\n",
        "        y[i]=network(x_temp, Theta)[0]\n",
        "  # plot network approximation\n",
        "    pyplot.plot(x,y,\"k-\", label=\"network\")\n",
        "    pyplot.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIIlR1TnNRMY"
      },
      "source": [
        "### Task 9\n",
        "\n",
        "For each of the datasets and their according optimized parameters, call the plotting function from Task 8. Use range $R=[-3,3]$ for dataset $X_1$ and $X_2$, and range $R=[-5.5,4.5]$ for dataset $X_3$. Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ the highest value for $x$. Did the networks approximate the functions? What can we do if not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df4qFFVVNRMY",
        "outputId": "e5d9ce1e-bc36-488b-8e01-3d5e6a798878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-7269b56cd9ad>:13: RuntimeWarning: overflow encountered in multiply\n",
            "  dw11=2/N*numpy.outer([(y-t)*w2*(1-h*h)],x)\n",
            "<ipython-input-3-7269b56cd9ad>:13: RuntimeWarning: invalid value encountered in multiply\n",
            "  dw11=2/N*numpy.outer([(y-t)*w2*(1-h*h)],x)\n",
            "<ipython-input-3-7269b56cd9ad>:14: RuntimeWarning: overflow encountered in multiply\n",
            "  dw22=2/N*(y-t)*w2*h\n",
            "<ipython-input-3-7269b56cd9ad>:16: RuntimeWarning: invalid value encountered in add\n",
            "  dw2=dw2+dw22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x216 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "figure = pyplot.figure(figsize=(10,3))\n",
        "Theta_network1=gradient_descent(X1, Theta1, eta=0.1)\n",
        "Theta_network2=gradient_descent(X2, Theta2, eta=0.1)\n",
        "Theta_network3=gradient_descent(X3, Theta3, eta=0.001)\n",
        "# plot first function\n",
        "pyplot.subplot(131)\n",
        "plot(X1, Theta_network1, (-3,3))\n",
        "\n",
        "# plot second function\n",
        "pyplot.subplot(132)\n",
        "plot(X2, Theta_network2, (-3,3))\n",
        "\n",
        "# plot third function\n",
        "pyplot.subplot(133)\n",
        "plot(X3, Theta_network3, (-5.5,4.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from the plot we can see the network can suit the shape of the data posiotions well. If not, it might be overfitting or underfitting. We can improve the network by regularizing the data, changing the data amout, modifying the Epoch or leaning rate."
      ],
      "metadata": {
        "id": "BnyOd_52shgM"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}